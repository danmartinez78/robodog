# Startup Validation Flow

## Overview

ShadowHound now has **two layers of LLM backend validation** to ensure reliable operation and fast failure detection.

## Two-Layer Validation

### Layer 1: Start Script Pre-Flight Check (`start.sh`)
**When**: Early in startup sequence, before any heavy initialization  
**Purpose**: Fail fast and save time

**Runs BEFORE**:
- Workspace build
- Dependency checks
- Robot driver launch
- Nav2 initialization

**What it checks**:

#### Ollama Backend
1. âœ… Service reachability (5s timeout)
2. âœ… Model availability (is model pulled?)
3. âœ… Test prompt (30s timeout)

#### OpenAI Backend
1. âœ… API key present
2. âœ… API key format check (starts with `sk-`)

**Example output**:
```bash
â”€â”€ LLM Backend Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â„¹ Configured backend: ollama
â„¹ Ollama URL: http://192.168.50.10:11434
â„¹ Ollama Model: qwen2.5-coder:32b

â„¹ 1. Checking Ollama service reachability...
âœ“ Ollama service responding
â„¹ 2. Checking if model 'qwen2.5-coder:32b' is available...
âœ“ Model 'qwen2.5-coder:32b' is available
â„¹ 3. Testing model response (this may take a few seconds)...
âœ“ Model responded successfully (3s)
â„¹ Response: OK

âœ“ Ollama backend validation passed!
```

**Time saved on failure**: 30-60 seconds (no robot driver launch, no Nav2 init)

### Layer 2: Mission Agent Validation (`mission_agent.py`)
**When**: During mission agent initialization  
**Purpose**: Authoritative validation before accepting missions

**Runs AFTER**:
- MissionExecutor initialization
- DIMOS robot/skills setup

**What it checks**:

#### Ollama Backend
1. âœ… Service responding
2. âœ… Model available
3. âœ… Test prompt succeeds

#### OpenAI Backend
1. âœ… API key present
2. âœ… Test prompt to OpenAI API

**Example output**:
```bash
============================================================
ğŸ” VALIDATING LLM BACKEND CONNECTION
============================================================
Testing ollama backend...
  URL: http://192.168.50.10:11434
  Model: qwen2.5-coder:32b
  Checking Ollama service...
  âœ… Ollama service responding
  âœ… Model 'qwen2.5-coder:32b' available
  Sending test prompt...
  âœ… Test prompt succeeded (response: 'OK')
============================================================
âœ… Ollama backend validation PASSED
============================================================
```

**Fail behavior**: Node exits with RuntimeError and clear error message

## Why Two Layers?

### Start Script Validation (Layer 1)
**Pros**:
- âœ… Catches issues **before** heavy system initialization
- âœ… Saves 30-60s by failing fast
- âœ… Better UX - user doesn't wait through driver launch
- âœ… Can continue on failure (user choice)

**Cons**:
- âš ï¸ Not authoritative (mission agent can still fail)
- âš ï¸ Adds 2-5s to startup (but saves 30-60s on failure)

### Mission Agent Validation (Layer 2)
**Pros**:
- âœ… Authoritative check - runs in mission agent context
- âœ… Validates actual LLM client initialization
- âœ… Cannot be bypassed
- âœ… Runs every time, even if start.sh skipped

**Cons**:
- âš ï¸ Happens after system initialization (later in sequence)

## Startup Sequence

```
./start.sh
  â†“
1. System checks (ROS2, Python, colcon)
  â†“
2. Git updates check
  â†“
3. Configuration setup (.env loading)
  â†“
4. ğŸ” LLM Backend Validation â† EARLY CHECK (Layer 1)
  â†“                              Fails fast, saves time
5. Workspace build
  â†“
6. Dependency checks
  â†“
7. Network checks
  â†“
8. Robot driver launch (30s)
  â†“
9. Nav2 initialization (15s)
  â†“
10. Mission agent launch
  â†“
11. MissionExecutor initialization
  â†“
12. ğŸ” Backend Validation â† AUTHORITATIVE (Layer 2)
  â†“                         Final check before accepting missions
13. Ready to accept missions!
```

## Failure Scenarios

### Scenario 1: Ollama Service Down

**Layer 1** (start.sh):
```
âœ— Cannot reach Ollama service at http://192.168.50.10:11434

Possible issues:
  â€¢ Ollama service not running
  â€¢ Wrong URL (check OLLAMA_BASE_URL in .env)
  â€¢ Network/firewall blocking connection
  â€¢ Thor not powered on (if using remote Ollama)

To fix:
  â€¢ Check Ollama status: docker ps | grep ollama
  â€¢ Test manually: curl http://192.168.50.10:11434/api/tags
  â€¢ Update .env with correct OLLAMA_BASE_URL

Continue anyway? (Mission agent will fail) [y/N]:
```

**User saved**: 30-60s (no robot driver/Nav2 launch)

### Scenario 2: Model Not Pulled

**Layer 1** (start.sh):
```
âœ“ Ollama service responding
âœ— Model 'qwen2.5-coder:32b' not found in Ollama

Available models:
  â€¢ llama3.1:70b
  â€¢ phi4:14b
  â€¢ qwen2.5:14b

To fix:
  â€¢ Pull the model: ollama pull qwen2.5-coder:32b
  â€¢ Or update OLLAMA_MODEL in .env to use an available model

Continue anyway? (Mission agent will fail) [y/N]:
```

**User saved**: 30-60s

### Scenario 3: OpenAI Key Missing

**Layer 1** (start.sh):
```
âœ— OPENAI_API_KEY not set in environment

To fix:
  â€¢ Add OPENAI_API_KEY to .env file
  â€¢ Get API key from: https://platform.openai.com/api-keys
```

**User saved**: 30-60s + clearer error message

### Scenario 4: Backend Reachable but Misconfigured

**Layer 1** passes (service is up, model exists)  
**Layer 2** fails (e.g., model incompatible with DIMOS)

Mission agent catches the actual runtime issue:
```
[ERROR] [mission_agent]: LLM backend validation failed
[ERROR] Model initialization failed: <specific error>
```

This is OK - Layer 2 provides final authoritative validation.

## Configuration

### Environment Variables

Set in `.env` file:

```bash
# Backend selection
AGENT_BACKEND=ollama  # or 'openai'

# Ollama configuration
OLLAMA_BASE_URL=http://192.168.50.10:11434
OLLAMA_MODEL=qwen2.5-coder:32b

# OpenAI configuration
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o
```

### Skipping Validation

**Not recommended**, but if you need to bypass for testing:

#### Skip start.sh validation
Comment out the check in `start.sh`:
```bash
# check_llm_backend  # Skipped for testing
```

#### Skip mission agent validation
Comment out in `mission_agent.py`:
```python
# if not self._validate_llm_backend():
#     raise RuntimeError(...)
```

âš ï¸ **Warning**: Skipping validation can lead to cryptic runtime errors.

## Performance Impact

### Layer 1 (start.sh)
- **Success case**: +2-5s startup time
- **Failure case**: Saves 30-60s by failing fast

**Net benefit**: Saves 25-55s on misconfiguration

### Layer 2 (mission_agent)
- **Always runs**: +2-5s agent initialization
- **Provides**: Authoritative validation, cannot be bypassed

**Total overhead**: 4-10s (acceptable for reliability gain)

## Testing Validation

### Test start.sh validation
```bash
# Test with unreachable backend
OLLAMA_BASE_URL=http://192.168.99.99:11434 ./start.sh

# Test with missing model
OLLAMA_MODEL=nonexistent-model ./start.sh

# Test OpenAI without key
unset OPENAI_API_KEY
AGENT_BACKEND=openai ./start.sh
```

### Test mission agent validation
```bash
# Launch mission agent directly (skips start.sh)
ros2 launch shadowhound_mission_agent mission_agent.launch.py \
    agent_backend:=ollama \
    ollama_base_url:=http://192.168.99.99:11434

# Should fail with clear error message
```

## Related Documentation

- **Backend Validation**: `docs/LLM_BACKEND_VALIDATION.md`
- **Implementation Summary**: `docs/BACKEND_VALIDATION_SUMMARY.md`
- **Deployment Checklist**: `docs/OLLAMA_DEPLOYMENT_CHECKLIST.md`

## Commits

- Layer 1 (start.sh): `0b22643` - Early validation in start script
- Layer 2 (mission_agent): `ef0138e` - Automatic validation on startup

---

**Last Updated**: 2025-10-10  
**Status**: âœ… Implemented and tested
